{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import pathlib\n",
    "import string\n",
    "from collections import Counter\n",
    "from natsort import natsorted\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global data_dir\n",
    "data_dir = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dir_path):\n",
    "    # read all files based on given input dir path\n",
    "    # INPUT: dataset path\n",
    "    # OUTPUT: dataset\n",
    "    data = []\n",
    "    temp = []\n",
    "    # asign the path\n",
    "    source_dir = pathlib.Path(dir_path)\n",
    "    \n",
    "    # read the path\n",
    "    for file in source_dir.iterdir():\n",
    "        temp.append(file.name)\n",
    "#         print(\"Dataset found on: \", file)\n",
    "    \n",
    "    x = natsorted(temp)\n",
    "    for item in x:\n",
    "        y = str(dir_path) + '/' + str(item)\n",
    "        data_dir.append(y)\n",
    "    \n",
    "    # read the files\n",
    "    for item in data_dir:\n",
    "        print(item)\n",
    "        temp = open(item, \"r\", encoding=\"utf-8\")\n",
    "        row = temp.read()\n",
    "        data.append(row)\n",
    "        temp.close()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grp_tokenization(dataset):\n",
    "    # tokenization the merged dataset\n",
    "    # INPUT: dataset string\n",
    "    # OUTPUT: tokened dataset list\n",
    "    \n",
    "    # remove punctuation from the dataset\n",
    "    temp = dataset.lower()\n",
    "    punctuation_trimmer = temp.translate(str.maketrans('','',string.punctuation))\n",
    "    return(nltk.word_tokenize(punctuation_trimmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(txtFile):\n",
    "    # tokenization the dataset array\n",
    "    # INPUT: dataset array\n",
    "    # OUTPUT: tokened dataset array\n",
    "    tok_data = []\n",
    "    \n",
    "    # remove punctuation from the dataset\n",
    "    for item in txtFile:\n",
    "        tok_data.append(grp_tokenization(item))\n",
    "    \n",
    "    return tok_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_GrpCounter(tokens):\n",
    "    # count the tokens in the token dataset\n",
    "    # INPUT: token\n",
    "    # OUTPUT: counted token list\n",
    "    return Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_counter(tokens):\n",
    "    # count the tokens in the token dataset\n",
    "    # INPUT: token array\n",
    "    # OUTPUT: counted token array\n",
    "    count = []\n",
    "    \n",
    "    for item in tokens:\n",
    "        count.append(form_GrpCounter(item))\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merger_dataset(dataset):\n",
    "    # merge all files inside the given dataset\n",
    "    # INPUT: dataset array\n",
    "    # OUTPUT: dataset string\n",
    "    data = \"\"\n",
    "    for txt in dataset:\n",
    "        data = data + txt\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data/a1.txt\n",
      "./Data/a2.txt\n",
      "./Data/a3.txt\n",
      "./Data/a4.txt\n",
      "./Data/a5.txt\n",
      "./Data/a6.txt\n",
      "./Data/a7.txt\n",
      "./Data/a8.txt\n",
      "./Data/a9.txt\n",
      "./Data/a10.txt\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"./Data\"\n",
    "dataset = read_data(dataset_path)\n",
    "tokens = tokenization(dataset)\n",
    "count = form_counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for item in count:\n",
    "#     for key, val in item.items():\n",
    "#         print(key, \", \", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data/a1.txt  has:  1672  of tokens\n",
      "./Data/a2.txt  has:  4223  of tokens\n",
      "./Data/a3.txt  has:  1905  of tokens\n",
      "./Data/a4.txt  has:  2885  of tokens\n",
      "./Data/a5.txt  has:  2998  of tokens\n",
      "./Data/a6.txt  has:  1542  of tokens\n",
      "./Data/a7.txt  has:  1421  of tokens\n",
      "./Data/a8.txt  has:  1591  of tokens\n",
      "./Data/a9.txt  has:  1401  of tokens\n",
      "./Data/a10.txt  has:  1247  of tokens\n"
     ]
    }
   ],
   "source": [
    "temp_counter = 0\n",
    "\n",
    "for item in tokens:\n",
    "    print(data_dir[temp_counter], \" has: \", len(item), \" of tokens\")\n",
    "    temp_counter = temp_counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 ans: above is the number of tokens in each doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Data/a1.txt  has:  612  of unique tokens\n",
      "./Data/a2.txt  has:  1249  of unique tokens\n",
      "./Data/a3.txt  has:  706  of unique tokens\n",
      "./Data/a4.txt  has:  969  of unique tokens\n",
      "./Data/a5.txt  has:  1050  of unique tokens\n",
      "./Data/a6.txt  has:  615  of unique tokens\n",
      "./Data/a7.txt  has:  591  of unique tokens\n",
      "./Data/a8.txt  has:  665  of unique tokens\n",
      "./Data/a9.txt  has:  568  of unique tokens\n",
      "./Data/a10.txt  has:  536  of unique tokens\n"
     ]
    }
   ],
   "source": [
    "temp_counter = 0\n",
    "\n",
    "for item in count:\n",
    "    print(data_dir[temp_counter], \" has: \", len(item), \" of unique tokens\")\n",
    "    temp_counter = temp_counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 ans: above is number of unique tokens in each doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = merger_dataset(dataset)\n",
    "merged_token = grp_tokenization(merged_dataset)\n",
    "merged_count = form_GrpCounter(merged_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20885"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 ans: There are total 20885 tokens in the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3792"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 ans: There are total 3792 unique tokens in the entire corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "////////////////////////////////////////////////// End #1 //////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopWrd_trimmer(dataset):\n",
    "    # remove stop words from the dataset\n",
    "    # INPUT: dataset array\n",
    "    # OUTPUT: stop-words-removed dataset array\n",
    "    stopWrd = []\n",
    "    for file in dataset:\n",
    "        stopWrd.append([wrd for wrd in file if not wrd in stopwords.words('english')])\n",
    "\n",
    "    return stopWrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grp_stopWrd_trimmer(dataset):\n",
    "    # remove stop words from the dataset\n",
    "    # INPUT: merged dataset\n",
    "    # OUTPUT: stop-words-removed dataset\n",
    "    return ([wrd for wrd in dataset if not wrd in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop word for the doc collection\n",
    "stopWrd = stopWrd_trimmer(tokens)\n",
    "# stop word for the corpus\n",
    "stopWrd_grp = grp_stopWrd_trimmer(merged_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop-words-removed for ./Data/a1.txt  has:  972  of tokens\n",
      "Stop-words-removed for ./Data/a2.txt  has:  2417  of tokens\n",
      "Stop-words-removed for ./Data/a3.txt  has:  1096  of tokens\n",
      "Stop-words-removed for ./Data/a4.txt  has:  1715  of tokens\n",
      "Stop-words-removed for ./Data/a5.txt  has:  1787  of tokens\n",
      "Stop-words-removed for ./Data/a6.txt  has:  910  of tokens\n",
      "Stop-words-removed for ./Data/a7.txt  has:  842  of tokens\n",
      "Stop-words-removed for ./Data/a8.txt  has:  960  of tokens\n",
      "Stop-words-removed for ./Data/a9.txt  has:  827  of tokens\n",
      "Stop-words-removed for ./Data/a10.txt  has:  727  of tokens\n"
     ]
    }
   ],
   "source": [
    "temp_counter = 0\n",
    "\n",
    "for item in stopWrd:\n",
    "    print(\"Stop-words-removed for\", data_dir[temp_counter], \" has: \", len(item), \" of tokens\")\n",
    "    temp_counter = temp_counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 ans: above is the total tokens in each stop-word doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form collection counter for the stop word doc collection\n",
    "stopWrd_count = form_counter(stopWrd)\n",
    "# form collection unique counter for the stop word doc collection\n",
    "grp_stopWrd_count = form_GrpCounter(stopWrd_grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop-words-removed for  ./Data/a1.txt  has:  535  of unique tokens\n",
      "Stop-words-removed for  ./Data/a2.txt  has:  1145  of unique tokens\n",
      "Stop-words-removed for  ./Data/a3.txt  has:  636  of unique tokens\n",
      "Stop-words-removed for  ./Data/a4.txt  has:  881  of unique tokens\n",
      "Stop-words-removed for  ./Data/a5.txt  has:  965  of unique tokens\n",
      "Stop-words-removed for  ./Data/a6.txt  has:  540  of unique tokens\n",
      "Stop-words-removed for  ./Data/a7.txt  has:  524  of unique tokens\n",
      "Stop-words-removed for  ./Data/a8.txt  has:  589  of unique tokens\n",
      "Stop-words-removed for  ./Data/a9.txt  has:  491  of unique tokens\n",
      "Stop-words-removed for  ./Data/a10.txt  has:  464  of unique tokens\n"
     ]
    }
   ],
   "source": [
    "temp_counter = 0\n",
    "\n",
    "for item in stopWrd_count:\n",
    "    print(\"Stop-words-removed for \", data_dir[temp_counter], \" has: \", len(item), \" of unique tokens\")\n",
    "    temp_counter = temp_counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 ans: above is the total unique tokens in each stop-word doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12253"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopWrd_grp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 ans: there are total 12826 tokens in the entire stop-words corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3678"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grp_stopWrd_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 ans: there are total 3678 unique tokens in the entire stop-words corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "////////////////////////////////////////////////// End #2 //////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(tokens, stemmer):\n",
    "    # stemming the files in the dataset\n",
    "    # INPUT: merged stop-words array and stemmer\n",
    "    # OUTPUT: stemming array\n",
    "    stemmed = []\n",
    "    \n",
    "    for file in tokens:\n",
    "        stemmed.append(grp_stemming(file, stemmer))\n",
    "            \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grp_stemming(tokens, stemmer):\n",
    "    # stemming the files in the dataset\n",
    "    # INPUT: merged stop-words and stemmer\n",
    "    # OUTPUT: stemming array\n",
    "    stemmed = []\n",
    "    \n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "            \n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming for the collection\n",
    "stem = stemming(stopWrd, stemmer)\n",
    "\n",
    "# stemming of the corpus\n",
    "grp_stem = grp_stemming(stopWrd_grp, stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for  ./Data/a1.txt  has:  972  of terms\n",
      "Stemming for  ./Data/a2.txt  has:  2417  of terms\n",
      "Stemming for  ./Data/a3.txt  has:  1096  of terms\n",
      "Stemming for  ./Data/a4.txt  has:  1715  of terms\n",
      "Stemming for  ./Data/a5.txt  has:  1787  of terms\n",
      "Stemming for  ./Data/a6.txt  has:  910  of terms\n",
      "Stemming for  ./Data/a7.txt  has:  842  of terms\n",
      "Stemming for  ./Data/a8.txt  has:  960  of terms\n",
      "Stemming for  ./Data/a9.txt  has:  827  of terms\n",
      "Stemming for  ./Data/a10.txt  has:  727  of terms\n"
     ]
    }
   ],
   "source": [
    "temp_counter = 0\n",
    "\n",
    "for item in stem:\n",
    "    print(\"Stemming for \", data_dir[temp_counter], \" has: \", len(item), \" of terms\")\n",
    "    temp_counter = temp_counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12253"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grp_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3 ans: there are of 12253 total vocabulary size for the entire corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "////////////////////////////////////////////////// End #3 //////////////////////////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    lower = text.lower()\n",
    "    trimmer = lower.translate(str.maketrans('','',string.punctuation))\n",
    "    tokens = nltk.word_tokenize(trimmer)\n",
    "    stopWrds = grp_stopWrd_trimmer(tokens)\n",
    "    stems = stem_tokens(stopWrds, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVec = TfidfVectorizer(tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(textlist):\n",
    "    tfidf = TfidfVec.fit_transform(textlist)\n",
    "    return (tfidf * tfidf.T).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cossim = cos_similarity(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for x in range(0, len(data_dir)):\n",
    "    df[data_dir[x]] = cossim[x]\n",
    "\n",
    "df.index = data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>./Data/a1.txt</th>\n",
       "      <th>./Data/a2.txt</th>\n",
       "      <th>./Data/a3.txt</th>\n",
       "      <th>./Data/a4.txt</th>\n",
       "      <th>./Data/a5.txt</th>\n",
       "      <th>./Data/a6.txt</th>\n",
       "      <th>./Data/a7.txt</th>\n",
       "      <th>./Data/a8.txt</th>\n",
       "      <th>./Data/a9.txt</th>\n",
       "      <th>./Data/a10.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>./Data/a1.txt</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.191569</td>\n",
       "      <td>0.138107</td>\n",
       "      <td>0.183352</td>\n",
       "      <td>0.111067</td>\n",
       "      <td>0.145551</td>\n",
       "      <td>0.144206</td>\n",
       "      <td>0.110951</td>\n",
       "      <td>0.109330</td>\n",
       "      <td>0.137202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./Data/a2.txt</th>\n",
       "      <td>0.191569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.210857</td>\n",
       "      <td>0.349566</td>\n",
       "      <td>0.191930</td>\n",
       "      <td>0.233374</td>\n",
       "      <td>0.212511</td>\n",
       "      <td>0.166300</td>\n",
       "      <td>0.154452</td>\n",
       "      <td>0.225532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./Data/a3.txt</th>\n",
       "      <td>0.138107</td>\n",
       "      <td>0.210857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.178857</td>\n",
       "      <td>0.128805</td>\n",
       "      <td>0.158293</td>\n",
       "      <td>0.142410</td>\n",
       "      <td>0.125205</td>\n",
       "      <td>0.109086</td>\n",
       "      <td>0.143745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./Data/a4.txt</th>\n",
       "      <td>0.183352</td>\n",
       "      <td>0.349566</td>\n",
       "      <td>0.178857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.182552</td>\n",
       "      <td>0.170691</td>\n",
       "      <td>0.180399</td>\n",
       "      <td>0.127223</td>\n",
       "      <td>0.119803</td>\n",
       "      <td>0.168167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./Data/a5.txt</th>\n",
       "      <td>0.111067</td>\n",
       "      <td>0.191930</td>\n",
       "      <td>0.128805</td>\n",
       "      <td>0.182552</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.146551</td>\n",
       "      <td>0.124117</td>\n",
       "      <td>0.102419</td>\n",
       "      <td>0.084690</td>\n",
       "      <td>0.119413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./Data/a6.txt</th>\n",
       "      <td>0.145551</td>\n",
       "      <td>0.233374</td>\n",
       "      <td>0.158293</td>\n",
       "      <td>0.170691</td>\n",
       "      <td>0.146551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.147908</td>\n",
       "      <td>0.131506</td>\n",
       "      <td>0.106841</td>\n",
       "      <td>0.154747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./Data/a7.txt</th>\n",
       "      <td>0.144206</td>\n",
       "      <td>0.212511</td>\n",
       "      <td>0.142410</td>\n",
       "      <td>0.180399</td>\n",
       "      <td>0.124117</td>\n",
       "      <td>0.147908</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.118701</td>\n",
       "      <td>0.100469</td>\n",
       "      <td>0.139972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./Data/a8.txt</th>\n",
       "      <td>0.110951</td>\n",
       "      <td>0.166300</td>\n",
       "      <td>0.125205</td>\n",
       "      <td>0.127223</td>\n",
       "      <td>0.102419</td>\n",
       "      <td>0.131506</td>\n",
       "      <td>0.118701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.087759</td>\n",
       "      <td>0.111443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./Data/a9.txt</th>\n",
       "      <td>0.109330</td>\n",
       "      <td>0.154452</td>\n",
       "      <td>0.109086</td>\n",
       "      <td>0.119803</td>\n",
       "      <td>0.084690</td>\n",
       "      <td>0.106841</td>\n",
       "      <td>0.100469</td>\n",
       "      <td>0.087759</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.102331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./Data/a10.txt</th>\n",
       "      <td>0.137202</td>\n",
       "      <td>0.225532</td>\n",
       "      <td>0.143745</td>\n",
       "      <td>0.168167</td>\n",
       "      <td>0.119413</td>\n",
       "      <td>0.154747</td>\n",
       "      <td>0.139972</td>\n",
       "      <td>0.111443</td>\n",
       "      <td>0.102331</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ./Data/a1.txt  ./Data/a2.txt  ./Data/a3.txt  ./Data/a4.txt  \\\n",
       "./Data/a1.txt        1.000000       0.191569       0.138107       0.183352   \n",
       "./Data/a2.txt        0.191569       1.000000       0.210857       0.349566   \n",
       "./Data/a3.txt        0.138107       0.210857       1.000000       0.178857   \n",
       "./Data/a4.txt        0.183352       0.349566       0.178857       1.000000   \n",
       "./Data/a5.txt        0.111067       0.191930       0.128805       0.182552   \n",
       "./Data/a6.txt        0.145551       0.233374       0.158293       0.170691   \n",
       "./Data/a7.txt        0.144206       0.212511       0.142410       0.180399   \n",
       "./Data/a8.txt        0.110951       0.166300       0.125205       0.127223   \n",
       "./Data/a9.txt        0.109330       0.154452       0.109086       0.119803   \n",
       "./Data/a10.txt       0.137202       0.225532       0.143745       0.168167   \n",
       "\n",
       "                ./Data/a5.txt  ./Data/a6.txt  ./Data/a7.txt  ./Data/a8.txt  \\\n",
       "./Data/a1.txt        0.111067       0.145551       0.144206       0.110951   \n",
       "./Data/a2.txt        0.191930       0.233374       0.212511       0.166300   \n",
       "./Data/a3.txt        0.128805       0.158293       0.142410       0.125205   \n",
       "./Data/a4.txt        0.182552       0.170691       0.180399       0.127223   \n",
       "./Data/a5.txt        1.000000       0.146551       0.124117       0.102419   \n",
       "./Data/a6.txt        0.146551       1.000000       0.147908       0.131506   \n",
       "./Data/a7.txt        0.124117       0.147908       1.000000       0.118701   \n",
       "./Data/a8.txt        0.102419       0.131506       0.118701       1.000000   \n",
       "./Data/a9.txt        0.084690       0.106841       0.100469       0.087759   \n",
       "./Data/a10.txt       0.119413       0.154747       0.139972       0.111443   \n",
       "\n",
       "                ./Data/a9.txt  ./Data/a10.txt  \n",
       "./Data/a1.txt        0.109330        0.137202  \n",
       "./Data/a2.txt        0.154452        0.225532  \n",
       "./Data/a3.txt        0.109086        0.143745  \n",
       "./Data/a4.txt        0.119803        0.168167  \n",
       "./Data/a5.txt        0.084690        0.119413  \n",
       "./Data/a6.txt        0.106841        0.154747  \n",
       "./Data/a7.txt        0.100469        0.139972  \n",
       "./Data/a8.txt        0.087759        0.111443  \n",
       "./Data/a9.txt        1.000000        0.102331  \n",
       "./Data/a10.txt       0.102331        1.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
